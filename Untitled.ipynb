{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as ii\n",
    "transforms_ = [ transforms.Resize(int(256*1.12), Image.BICUBIC), \n",
    "                transforms.RandomCrop(256), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "transform = transforms.Compose(transforms_)\n",
    "transform(Image.open(\"datasets/vangogh2photo/trainA/00001.jpg\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import struct\n",
    "#from skimage.io import imread\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "transforms_ = [ transforms.Resize(int(256*1.12), Image.BICUBIC), \n",
    "                transforms.RandomCrop(256), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "transform = transforms.Compose(transforms_)\n",
    "\n",
    "def load_data(nr_of_channels,videosequence, batch_size=1, nr_A_train_imgs=None, nr_B_train_imgs=None,\n",
    "              nr_A_test_imgs=None, nr_B_test_imgs=None, subfolder='',\n",
    "              generator=False, D_model=None, use_multiscale_discriminator=False, use_supervised_learning=False, REAL_LABEL=1.0):\n",
    "\n",
    "#     trainA_path = os.path.join('datasets', subfolder, 'trainA')\n",
    "    trainB_path = os.path.join('datasets', subfolder, 'trainA')\n",
    "#     testA_path = os.path.join('datasets', subfolder, 'testA')\n",
    "    testB_path = os.path.join('datasets', subfolder, 'testA')\n",
    "    \n",
    "    video_frames_path = os.path.join('Videos','Frames',str(videosequence))\n",
    "\n",
    "#     trainA_image_names = os.listdir(trainA_path)\n",
    "    frame_names = os.listdir(video_frames_path)\n",
    "#     if nr_A_train_imgs != None:\n",
    "#         trainA_image_names = trainA_image_names[:nr_A_train_imgs]\n",
    "\n",
    "    trainB_image_names = os.listdir(trainB_path)\n",
    "#     if nr_B_train_imgs != None:\n",
    "#         trainB_image_names = trainB_image_names[:nr_B_train_imgs]\n",
    "\n",
    "#     testA_image_names = os.listdir(testA_path)\n",
    "#     if nr_A_test_imgs != None:\n",
    "#         testA_image_names = testA_image_names[:nr_A_test_imgs]\n",
    "\n",
    "    testB_image_names = os.listdir(testB_path)\n",
    "    if nr_B_test_imgs != None:\n",
    "        testB_image_names = testB_image_names[:nr_B_test_imgs]\n",
    "\n",
    "    if generator:\n",
    "        return data_sequence(video_frames_path, trainB_path, frame_names, trainB_image_names, batch_size=batch_size)  # D_model, use_multiscale_discriminator, use_supervised_learning, REAL_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_flow_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        # 4 bytes header\n",
    "        header = struct.unpack('4s', f.read(4))[0]\n",
    "        # 4 bytes width, height    \n",
    "        w = struct.unpack('i', f.read(4))[0]\n",
    "        h = struct.unpack('i', f.read(4))[0]   \n",
    "        flow = np.ndarray((2, h, w), dtype=np.float32)\n",
    "        for y in range(h):\n",
    "            for x in range(w):\n",
    "                flow[0,y,x] = struct.unpack('f', f.read(4))[0]\n",
    "                flow[1,y,x] = struct.unpack('f', f.read(4))[0]\n",
    "    return flow\n",
    "\n",
    "\n",
    "\n",
    "class data_sequence(Dataset):\n",
    "\n",
    "    def __init__(self, trainA_path, trainB_path, image_list_A, image_list_B, batch_size=1):  # , D_model, use_multiscale_discriminator, use_supervised_learning, REAL_LABEL):\n",
    "        self.batch_size = batch_size\n",
    "        self.frames = []\n",
    "        self.train_B = []\n",
    "        self.frames_path = trainA_path\n",
    "        for image_name in image_list_A:\n",
    "            if image_name[-1].lower() == 'g':  # to avoid e.g. thumbs.db files\n",
    "                self.frames.append(os.path.join(trainA_path, image_name))\n",
    "        for image_name in image_list_B:\n",
    "            if image_name[-1].lower() == 'g':  # to avoid e.g. thumbs.db files\n",
    "                self.train_B.append(os.path.join(trainB_path, image_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.frames)) -1\n",
    "\n",
    "    def __getitem__(self, idx):  # , use_multiscale_discriminator, use_supervised_learning):if loop_index + batch_size >= min_nr_imgs:\n",
    "        \n",
    "        frameT_path = os.path.join(self.frames_path,\"frame\"+str(idx+1)+\".jpg\")\n",
    "        frameT_1_path = os.path.join(self.frames_path,\"frame\"+str(idx)+\".jpg\")\n",
    "        flow_path = os.path.join(self.frames_path,\"flow\",str(idx)+\"_\"+str(idx+1)+\"_forward.flo\")\n",
    "        mask_path = os.path.join(self.frames_path,\"flow\",\"reliable_\"+str(idx)+\"_\"+str(idx+1)+\".pgm\")\n",
    "        frame_T = Image.open(frameT_path)\n",
    "        frame_T_1 = Image.open(frameT_1_path)\n",
    "        flow = read_flow_file(flow_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        zero=np.where((mask[:,:,0]<254) & (mask[:,:,1]<254) & (mask[:,:,2]<254))\n",
    "        one = np.where((mask[:,:,0]>=254) & (mask[:,:,1]>=254) & (mask[:,:,2]>=254))\n",
    "        mask[one]=(1,1,1)\n",
    "        mask[zero]=(0,0,0)\n",
    "        index_B = np.random.randint(len(self.train_B))\n",
    "        real_B = Image.open(self.train_B[index_B])\n",
    "        return(frameT_1_path,frameT_path,flow_path,mask_path)\n",
    "        return (transform(frame_T),transform(frame_T_1),torch.Tensor(flow),torch.Tensor(mask),transform(real_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = load_data(nr_of_channels=3, batch_size=1,videosequence=1, generator=True, subfolder='vangogh2photo')\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dg:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "dataloaders = []\n",
    "for i in range(1,10):\n",
    "    dataloaders.append(DataLoader(datasets.load_data(\n",
    "        nr_of_channels=3, batch_size=1,videosequence=i, generator=True, subfolder='vangogh2photo'),\n",
    "                                  batch_size=1, shuffle=False, num_workers=8))\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders[0][0]['frame_T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "to_pil = torchvision.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "for d in dataloaders[0]:\n",
    "    frame_t_1 = d['frame_T_1']\n",
    "    flow = d['flow']\n",
    "    print(d['mask'].shape)\n",
    "#     truncateAndSave(d['frame_T'],d['frame_T'],d['frame_T'],'images/1.png')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "dg = datasets.load_data(nr_of_channels=3, batch_size=1,videosequence=1, generator=True, subfolder='vangogh2photo')\n",
    "for d in dg:\n",
    "    print(d[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import ImageDataset\n",
    "dataloader = DataLoader(dg, \n",
    "                        batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "def warp_image(x, flo):\n",
    "        \"\"\"\n",
    "        warp an image/tensor (im2) back to im1, according to the optical flow\n",
    "        x: [B, C, H, W] (im2)\n",
    "        flo: [B, 2, H, W] flow\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.size()\n",
    "        # mesh grid \n",
    "        xx = torch.arange(0, W).view(1,-1).repeat(H,1)\n",
    "        yy = torch.arange(0, H).view(-1,1).repeat(1,W)\n",
    "        xx = xx.view(1,1,H,W).repeat(B,1,1,1)\n",
    "        yy = yy.view(1,1,H,W).repeat(B,1,1,1)\n",
    "        grid = torch.cat((xx,yy),1).float()\n",
    "\n",
    "        if x.is_cuda:\n",
    "            grid = grid.cuda()\n",
    "        vgrid = Variable(grid) + flo\n",
    "\n",
    "        # scale grid to [-1,1] \n",
    "        vgrid[:,0,:,:] = 2.0*vgrid[:,0,:,:]/max(W-1,1)-1.0\n",
    "        vgrid[:,1,:,:] = 2.0*vgrid[:,1,:,:]/max(H-1,1)-1.0\n",
    "\n",
    "        vgrid = vgrid.permute(0,2,3,1)        \n",
    "        output = torch.nn.functional.grid_sample(x, vgrid)\n",
    "        mask = torch.autograd.Variable(torch.ones(x.size())).cuda()\n",
    "        mask = torch.nn.functional.grid_sample(mask, vgrid)\n",
    "\n",
    "        # if W==128:\n",
    "            # np.save('mask.npy', mask.cpu().data.numpy())\n",
    "            # np.save('warp.npy', output.cpu().data.numpy())\n",
    "        \n",
    "        mask[mask<0.9999] = 0\n",
    "        mask[mask>0] = 1\n",
    "        \n",
    "        return output *mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(fake_frame_t, warped_frame, c):\n",
    "    D = warped_frame.shape[1] * warped_frame.shape[2] * warped_frame.shape[3]\n",
    "    loss = (1./D) *torch.sum(c * (fake_frame_t - warped_frame)**2)\n",
    "    return loss*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torchvision\n",
    "    import torch\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    def truncateAndSave( real, synthetic, reconstructed, path_name):\n",
    "        if len(real.shape) > 3:\n",
    "            real = to_pil(real[0].detach().cpu())\n",
    "            synthetic = to_pil(synthetic[0].detach().cpu())\n",
    "            reconstructed = to_pil(reconstructed[0].detach().cpu())\n",
    "\n",
    "        image = np.hstack((real, synthetic, reconstructed))\n",
    "        image = Image.fromarray(image)\n",
    "        image.save(path_name)\n",
    "\n",
    "    def saveImages( epoch, frame_t, frame_t_1):\n",
    "        directory = os.path.join('images', date_time)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "\n",
    "        if len(frame_t.shape) < 4:\n",
    "            frame_t = np.expand_dims(frame_t, axis=0)\n",
    "            frame_t_1 = np.expand_dims(frame_t_1, axis=0)\n",
    "\n",
    "        fake_frame_t = netG_A2B(frame_t)\n",
    "        fake_frame_t_1 = netG_A2B(frame_t_1)\n",
    "        reconstructed_frame_t = netG_B2A(fake_frame_t)\n",
    "        reconstructed_frame_t_1 = netG_B2A(fake_frame_t_1)\n",
    "\n",
    "        truncateAndSave( frame_t, fake_frame_t, reconstructed_frame_t,\n",
    "                             '{}/epoch{}_t.png'.format(\n",
    "                                 directory,str(epoch)))\n",
    "        truncateAndSave( frame_t_1, fake_frame_t_1, reconstructed_frame_t_1,\n",
    "                             '{}/epoch{}_t_1.png'.format(\n",
    "                                 directory, str(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Generator' from 'models' (G:\\Drive D\\Trinity Assignments\\Dissertation\\pytorch-CycleGAN-and-pix2pix-master\\models\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-13de163b610c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Generator' from 'models' (G:\\Drive D\\Trinity Assignments\\Dissertation\\pytorch-CycleGAN-and-pix2pix-master\\models\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from models import Generator\n",
    "from models import Discriminator\n",
    "from utils import ReplayBuffer\n",
    "from utils import LambdaLR\n",
    "from utils import Logger\n",
    "from utils import weights_init_normal\n",
    "import datasets\n",
    "import os\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--epoch', type=int, default=0, help='starting epoch')\n",
    "# parser.add_argument('--n_epochs', type=int, default=200, help='number of epochs of training')\n",
    "# parser.add_argument('--batchSize', type=int, default=1, help='size of the batches')\n",
    "# parser.add_argument('--dataroot', type=str, default='datasets/horse2zebra/', help='root directory of the dataset')\n",
    "# parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate')\n",
    "# parser.add_argument('--decay_epoch', type=int, default=100, help='epoch to start linearly decaying the learning rate to 0')\n",
    "# parser.add_argument('--size', type=int, default=256, help='size of the data crop (squared assumed)')\n",
    "# parser.add_argument('--input_nc', type=int, default=3, help='number of channels of input data')\n",
    "# parser.add_argument('--output_nc', type=int, default=3, help='number of channels of output data')\n",
    "# parser.add_argument('--cuda', default=True,action='store_true', help='use GPU computation')\n",
    "# parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)\n",
    "opt_epoch = 0\n",
    "opt_n_epochs = 100\n",
    "opt_batchSize = 1\n",
    "opt_dataroot = 'datasets/vangogh2photo/'\n",
    "opt_lr =0.0002 \n",
    "opt_decay_epoch = 50\n",
    "opt_size = 256\n",
    "opt_input_nc = 3\n",
    "opt_output_nc = 3\n",
    "opt_cuda = True\n",
    "opt_n_cpu = 1\n",
    "import time\n",
    "date_time = time.strftime('%Y%m%d-%H%M%S', time.localtime()) \n",
    "if torch.cuda.is_available() and not opt_cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "###### Definition of variables ######\n",
    "# Networks\n",
    "netG_A2B = Generator(opt_input_nc, opt_output_nc)\n",
    "netG_B2A = Generator(opt_output_nc, opt_input_nc)\n",
    "netD_A = Discriminator(opt_input_nc)\n",
    "netD_B = Discriminator(opt_output_nc)\n",
    "netG_A2B.load_state_dict(torch.load('output/netG_A2B.pth'))\n",
    "netG_B2A.load_state_dict(torch.load('output/netG_B2A.pth'))\n",
    "netD_A.load_state_dict(torch.load('output/netD_A.pth'))\n",
    "netD_B.load_state_dict(torch.load('output/netD_B.pth'))\n",
    "\n",
    "\n",
    "if opt_cuda:\n",
    "    netG_A2B.cuda()\n",
    "    netG_B2A.cuda()\n",
    "    netD_A.cuda()\n",
    "    netD_B.cuda()\n",
    "\n",
    "# netG_A2B.apply(weights_init_normal)\n",
    "# netG_B2A.apply(weights_init_normal)\n",
    "# netD_A.apply(weights_init_normal)\n",
    "# netD_B.apply(weights_init_normal)\n",
    "\n",
    "# for param in netG_B2A.parameters(): # don't train painting to photos, doesn't seem to work\n",
    "#     param.requires_grad = False\n",
    "# Lossess\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "criterion_temporal = weighted_mse_loss\n",
    "\n",
    "# Optimizers & LR schedulers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(),netG_B2A.parameters()),\n",
    "                                lr=opt_lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=opt_lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=opt_lr, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(opt_n_epochs, opt_epoch, opt_decay_epoch).step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(opt_n_epochs, opt_epoch, opt_decay_epoch).step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(opt_n_epochs, opt_epoch, opt_decay_epoch).step)\n",
    "\n",
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if opt_cuda else torch.Tensor\n",
    "# input_A = Tensor(opt_batchSize, opt_input_nc, opt_size, opt_size)\n",
    "frame_t = Tensor(opt_batchSize, opt_input_nc, opt_size, opt_size)\n",
    "frame_t_1 = Tensor(opt_batchSize, opt_input_nc, opt_size, opt_size)\n",
    "input_B = Tensor(opt_batchSize, opt_output_nc, opt_size, opt_size)\n",
    "target_real = Variable(Tensor(opt_batchSize).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(opt_batchSize).fill_(0.0), requires_grad=False)\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Dataset loader\n",
    "transforms_ = [ transforms.Resize(int(opt_size*1.12), Image.BICUBIC), \n",
    "                transforms.RandomCrop(opt_size), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "dataloaders = []\n",
    "for i in range(1,10):\n",
    "    dataloaders.append(DataLoader(datasets.load_data(\n",
    "        nr_of_channels=3, batch_size=1,videosequence=i, generator=True, subfolder='vangogh2photo'),\n",
    "                                 batch_size=opt_batchSize, shuffle=False, num_workers=opt_n_cpu))\n",
    "# dataloader = DataLoader(ImageDataset(opt_dataroot, transforms_=transforms_, unaligned=True), \n",
    "#                         batch_size=opt_batchSize, shuffle=True, num_workers=opt_n_cpu)\n",
    "\n",
    "# Loss plot\n",
    "logger = Logger(opt_n_epochs, sum([len(x) for x in dataloaders] ))\n",
    "###################################\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "###### Training ######\n",
    "for epoch in range(opt_epoch, opt_n_epochs):\n",
    "    for dataloader in dataloaders:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Set model input\n",
    "            real_frame_t = Variable(frame_t.copy_(batch['frame_T']))\n",
    "            real_frame_t_1 = Variable(frame_t_1.copy_(batch['frame_T_1']))\n",
    "            real_B = Variable(input_B.copy_(batch['real_B']))\n",
    "\n",
    "            ###### Generators A2B and B2A ######\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Identity loss\n",
    "            # G_A2B(B) should equal B if real B is fed\n",
    "            same_B = netG_A2B(real_B)\n",
    "            loss_identity_B = criterion_identity(same_B, real_B)*0\n",
    "#             G_B2A(A) should equal A if real A is fed\n",
    "            same_A = netG_B2A(real_frame_t)\n",
    "            loss_identity_A = criterion_identity(same_A, real_frame_t)*0\n",
    "\n",
    "            # GAN loss\n",
    "            fake_frame_t = netG_A2B(real_frame_t)\n",
    "            pred_fake = netD_B(fake_frame_t)\n",
    "            loss_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
    "            fake_frame_t_1 = netG_A2B(real_frame_t_1)\n",
    "\n",
    "            fake_A = netG_B2A(real_B)\n",
    "            pred_fake = netD_A(fake_A)\n",
    "            loss_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "            # Cycle loss\n",
    "            recovered_frame_t = netG_B2A(fake_frame_t)\n",
    "            loss_cycle_ABA = criterion_cycle(recovered_frame_t, real_frame_t)*10.0\n",
    "            \n",
    "            recovered_B = netG_A2B(fake_A)\n",
    "            loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * 10.0\n",
    "            \n",
    "            #Temporal Loss\n",
    "            warped_frame = warp_image(fake_frame_t_1,batch['flow'].cuda())\n",
    "#             warped_frame = torch.reshape(warped_frame,(1,3,256,256))\n",
    "            mask = batch['mask'].permute(0,3,1,2)\n",
    "#             mask = torch.reshape(batch['mask'], (1,3,256, 256))\n",
    "#             if torch.isnan(fake_frame_t):\n",
    "#                 print('invalid input detected in frame_t ', i)\n",
    "#             if torch.isnan(warped_frame):\n",
    "#                 print('invalid input detected in frame_t ', i)\n",
    "#             if torch.isnan(mask):\n",
    "#                 print('invalid input detected in frame_t ', i)\n",
    "            temporal_loss = criterion_temporal(fake_frame_t.cuda(),warped_frame,mask.cuda()) \n",
    "            \n",
    "#             temporal_loss = criterion_temporal(fake_frame_t.cuda(),warped_frame.cuda())\n",
    "            \n",
    "\n",
    "            \n",
    "            # Total loss\n",
    "            loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB +temporal_loss\n",
    "            loss_G.backward()\n",
    "\n",
    "            optimizer_G.step()\n",
    "            ###################################\n",
    "\n",
    "            ###### Discriminator A ######\n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            pred_real = netD_A(real_frame_t)\n",
    "            loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "\n",
    "            # Fake loss\n",
    "            fake_frame_t = fake_A_buffer.push_and_pop(fake_frame_t)\n",
    "            pred_fake = netD_A(fake_frame_t.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "            loss_D_A.backward()\n",
    "\n",
    "            optimizer_D_A.step()\n",
    "            ###################################\n",
    "\n",
    "            ###### Discriminator B ######\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            pred_real = netD_B(real_B)\n",
    "            loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "\n",
    "            # Fake loss\n",
    "            fake_B = fake_B_buffer.push_and_pop(fake_frame_t)\n",
    "            pred_fake = netD_B(fake_B.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "            loss_D_B.backward()\n",
    "\n",
    "            optimizer_D_B.step()\n",
    "            ###################################\n",
    "\n",
    "            # Progress report (http://localhost:8097)\n",
    "#             logger.log({'loss_G': loss_G, 'loss_G_identity': loss_identity_B, 'loss_G_GAN': loss_GAN_A2B ,\n",
    "#                         'loss_G_cycle': (loss_cycle_ABA + loss_cycle_BAB), 'loss_D': loss_D_B}, \n",
    "#                         images={'frame_t': frame_t, 'fram_t_1': frame_t_1, 'fake_frame_t': fake_frame_t, 'fake_frame_t_1': fake_frame_t_1})\n",
    "            print(\"epoch -----\", epoch,\"/\",opt_n_epochs)\n",
    "            print('loop------',i,\"/\",len(dataloader))\n",
    "            print('loss_G-----',float(loss_G.data))\n",
    "            print('loss_G_identity-----',float(loss_identity_B.data))\n",
    "            print('loss_G_GAN-----', float(loss_GAN_A2B.data))\n",
    "            print('loss_G_cycle-----',float(loss_cycle_ABA + loss_cycle_BAB.data))\n",
    "            print('loss_D-----',float(loss_D_B.data))\n",
    "            print('temporal_losss-----', float(temporal_loss.data))\n",
    "\n",
    "            if i%100 ==0:\n",
    "                print('\\n', '\\n', '-------------------------Saving images for batch')\n",
    "                saveImages(i, frame_t, frame_t_1)\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    # Save models checkpoints\n",
    "    torch.save(netG_A2B.state_dict(), 'output/netG_A2B.pth')\n",
    "    torch.save(netG_B2A.state_dict(), 'output/netG_B2A.pth')\n",
    "    torch.save(netD_A.state_dict(), 'output/netD_A.pth')\n",
    "    torch.save(netD_B.state_dict(), 'output/netD_B.pth')\n",
    "###################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def warp( x, flo):\n",
    "        \"\"\"\n",
    "        warp an image/tensor (im2) back to im1, according to the optical flow\n",
    "        x: [B, C, H, W] (im2)\n",
    "        flo: [B, 2, H, W] flow\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.size()\n",
    "        # mesh grid \n",
    "        xx = torch.arange(0, W).view(1,-1).repeat(H,1)\n",
    "        yy = torch.arange(0, H).view(-1,1).repeat(1,W)\n",
    "        xx = xx.view(1,1,H,W).repeat(B,1,1,1)\n",
    "        yy = yy.view(1,1,H,W).repeat(B,1,1,1)\n",
    "        grid = torch.cat((xx,yy),1).float()\n",
    "\n",
    "        if x.is_cuda:\n",
    "            grid = grid.cuda()\n",
    "        vgrid = Variable(grid) + flo\n",
    "\n",
    "        # scale grid to [-1,1] \n",
    "        vgrid[:,0,:,:] = 2.0*vgrid[:,0,:,:]/max(W-1,1)-1.0\n",
    "        vgrid[:,1,:,:] = 2.0*vgrid[:,1,:,:]/max(H-1,1)-1.0\n",
    "\n",
    "        vgrid = vgrid.permute(0,2,3,1)        \n",
    "        output = torch.nn.functional.grid_sample(x, vgrid)\n",
    "        mask = torch.autograd.Variable(torch.ones(x.size())).cuda()\n",
    "        mask = torch.nn.functional.grid_sample(mask, vgrid)\n",
    "\n",
    "        # if W==128:\n",
    "            # np.save('mask.npy', mask.cpu().data.numpy())\n",
    "            # np.save('warp.npy', output.cpu().data.numpy())\n",
    "        \n",
    "        mask[mask<0.9999] = 0\n",
    "        mask[mask>0] = 1\n",
    "        \n",
    "        return output *mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp(x,flo):\n",
    "    B,H,W,C = x.size()\n",
    "    # mesh grid\n",
    "    xx = torch.arange(0,W).view(1,-1).repeat(H,1)\n",
    "    yy = torch.arange(0,H).view(-1,1).repeat(1,W)\n",
    "\n",
    "    xx = xx.view(1,H,W,1).repeat(B,1,1,1)\n",
    "    yy = yy.view(1,H,W,1).repeat(B,1,1,1)\n",
    "\n",
    "\n",
    "    grid = torch.cat((xx,yy),3).float()\n",
    "    \n",
    "    if x.is_cuda:\n",
    "        grid = grid.cuda()\n",
    "    print(grid.shape)\n",
    "    print(flo.shape)\n",
    "    vgrid = Variable(grid) + flo\n",
    "    \n",
    "    ## scale grid to [-1,1]\n",
    "    vgrid[:,:,:,0] = 2.0*vgrid[:,:,:,0].clone()/max(W-1,1)-1.0\n",
    "    vgrid[:,:,:,1] = 2.0*vgrid[:,:,:,1].clone()/max(H-1,1)-1.0\n",
    "\n",
    "    x = x.permute(0,3,1,2)\n",
    "\n",
    "    output = torch.nn.functional.grid_sample(x,vgrid)\n",
    "    mask = torch.autograd.Variable(torch.ones(x.size()))\n",
    "    mask = torch.nn.functional.grid_sample(mask,vgrid)\n",
    "    \n",
    "    mask[mask<0.9999]=0\n",
    "    mask[mask>0]=1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_warp(x, flow, padding_mode='zeros'):\n",
    "    \"\"\"Warp an image or feature map with optical flow\n",
    "    Args:\n",
    "        x (Tensor): size (n, c, h, w)\n",
    "        flow (Tensor): size (n, 2, h, w), values range from -1 to 1 (relevant to image width or height)\n",
    "        padding_mode (str): 'zeros' or 'border'\n",
    "\n",
    "    Returns:\n",
    "        Tensor: warped image or feature map\n",
    "    \"\"\"\n",
    "    print(x.size()[-2:])\n",
    "    print(flow.size())\n",
    "    assert x.size()[-2:] == flow.size()[-2:]\n",
    "    n, _, h, w = x.size()\n",
    "    x_ = torch.arange(w).view(1, -1).expand(h, -1)\n",
    "    y_ = torch.arange(h).view(-1, 1).expand(-1, w)\n",
    "    grid = torch.stack([x_, y_], dim=0).float().cuda()\n",
    "    grid = grid.unsqueeze(0).expand(n, -1, -1, -1)\n",
    "    grid[:, 0, :, :] = 2 * grid[:, 0, :, :] / (w - 1) - 1\n",
    "    grid[:, 1, :, :] = 2 * grid[:, 1, :, :] / (h - 1) - 1\n",
    "    grid += 2 * flow\n",
    "    grid = grid.permute(0, 2, 3, 1)\n",
    "    return F.grid_sample(x, grid, padding_mode=padding_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cv2.resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_t_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from fastai.vision import *\n",
    "import numpy as np\n",
    "# test = cv2.readOpticalFlow('Videos/Frames/1/flow/0_1_forward.flo')\n",
    "# test = torch.from_numpy(test)\n",
    "# test = test.permute(2,0,1)\n",
    "# test = test[np.newaxis,:,:,:]\n",
    "# test = Variable(test).cuda()\n",
    "\n",
    "# test_img2 = cv2.imread('Videos/Frames/1/frame0.jpg')\n",
    "# test_img2 = cv2.resize(test_img2,(256,256))\n",
    "# test_img2 = torch.from_numpy(test_img2)\n",
    "# test_img2 = test_img2.permute(2,0,1)\n",
    "# test_img2 = np.float32(test_img2)\n",
    "# test_img2 = test_img2[np.newaxis,:,:,:]\n",
    "# test_img2 = torch.from_numpy(test_img2)\n",
    "# test_img2 = Variable(test_img2).cuda()\n",
    "\n",
    "out = warp(frame_t_1.cuda(), flow.cuda())\n",
    "out = out[0].permute(1,2,0)\n",
    "out = out.cpu()\n",
    "# out = np.float32(out)\n",
    "# cv2.imwrite('test3.png', out*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.isnan(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(frame_t_1[0].permute(1,2,0).cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
